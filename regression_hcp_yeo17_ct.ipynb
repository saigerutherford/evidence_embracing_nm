{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T21:49:43.572309Z",
     "start_time": "2020-02-27T21:49:43.566414Z"
    },
    "id": "IRG2nlngfOF_"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import stats, linalg\n",
    "from sklearn import preprocessing, decomposition, linear_model, metrics \n",
    "from sklearn.utils import shuffle\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "id": "HECqM4rZVcD9"
   },
   "outputs": [],
   "source": [
    "# Yeo functional networks\n",
    "hcp_yeo_z = np.load('data/hcp_yeo_z.npy')\n",
    "hcp_yeo_resid = np.load('data/hcp_yeo_resid.npy')\n",
    "hcp_yeo_g = np.load('data/hcp_yeo_g.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "id": "iho4wkAESgqf"
   },
   "outputs": [],
   "source": [
    "# generate train/test splits\n",
    "np.random.seed(42)\n",
    "n_train = int(0.9 * hcp_yeo_z.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "id": "zi7v5e8vZ0Ms"
   },
   "outputs": [],
   "source": [
    "train_idxs = np.random.choice(range(hcp_yeo_z.shape[0]), size=n_train, replace=False)\n",
    "test_idxs = np.array([x for x in range(hcp_yeo_z.shape[0]) if x not in train_idxs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "id": "zi7v5e8vZ0Ms"
   },
   "outputs": [],
   "source": [
    "train_data_z = hcp_yeo_z[train_idxs, :]\n",
    "test_data_z = hcp_yeo_z[test_idxs, :]\n",
    "\n",
    "train_data_raw = hcp_yeo_resid[train_idxs, :]\n",
    "test_data_raw = hcp_yeo_resid[test_idxs, :]\n",
    "\n",
    "train_phen = hcp_yeo_g[train_idxs]\n",
    "test_phen = hcp_yeo_g[test_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean center train/test data (using train means)\n",
    "train_mu_centered_z = (train_data_z - train_data_z.mean(axis=0))\n",
    "test_mu_centered_z = (test_data_z - train_data_z.mean(axis=0))\n",
    "\n",
    "train_mu_centered_raw = (train_data_raw - train_data_raw.mean(axis=0))\n",
    "test_mu_centered_raw = (test_data_raw - train_data_raw.mean(axis=0))\n",
    "\n",
    "# from pca documentation, \"the input data is centered but not scaled for each feature before applying the SVD\"\n",
    "pca_model_z = decomposition.PCA(n_components=15).fit(train_data_z)\n",
    "pca_model_raw = decomposition.PCA(n_components=15).fit(train_data_raw)\n",
    "\n",
    "train_transformed_z = pca_model_z.transform(train_data_z)\n",
    "test_transformed_z = pca_model_z.transform(test_data_z)\n",
    "train_transformed_raw = pca_model_raw.transform(train_data_raw)\n",
    "test_transformed_raw = pca_model_raw.transform(test_data_raw)\n",
    "\n",
    "# OLS using sklearn\n",
    "lr_model_z = linear_model.LinearRegression(fit_intercept=True, normalize=False)\n",
    "lr_model_z.fit(train_transformed_z, train_phen)\n",
    "train_pred_phen_lr_model_z = lr_model_z.predict(train_transformed_z)\n",
    "test_pred_phen_lr_model_z = lr_model_z.predict(test_transformed_z)\n",
    "\n",
    "# OLS using sklearn\n",
    "lr_model_raw = linear_model.LinearRegression(fit_intercept=True, normalize=False)\n",
    "lr_model_raw.fit(train_transformed_raw, train_phen)\n",
    "train_pred_phen_lr_model_raw = lr_model_raw.predict(train_transformed_raw)\n",
    "test_pred_phen_lr_model_raw = lr_model_raw.predict(test_transformed_raw)\n",
    "\n",
    "# HCP Accuracy of Predictions (deviations)\n",
    "train_r2_z = metrics.r2_score(train_phen, train_pred_phen_lr_model_z)\n",
    "test_r2_z = metrics.r2_score(test_phen, test_pred_phen_lr_model_z)\n",
    "train_mse_z = metrics.mean_squared_error(train_phen, train_pred_phen_lr_model_z)\n",
    "test_mse_z = metrics.mean_squared_error(test_phen, test_pred_phen_lr_model_z)\n",
    "\n",
    "# HCP Accuracy of Predictions (raw)\n",
    "train_r2_raw = metrics.r2_score(train_phen, train_pred_phen_lr_model_raw)\n",
    "test_r2_raw = metrics.r2_score(test_phen, test_pred_phen_lr_model_raw)\n",
    "train_mse_raw = metrics.mean_squared_error(train_phen, train_pred_phen_lr_model_raw)\n",
    "test_mse_raw = metrics.mean_squared_error(test_phen, test_pred_phen_lr_model_raw)\n",
    "\n",
    "# Difference between deviation and raw\n",
    "diff_test_r = test_r2_z - test_r2_raw\n",
    "diff_test_mse = test_mse_z - test_mse_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HECqM4rZVcD9"
   },
   "outputs": [],
   "source": [
    "# Cortical Thickness\n",
    "hcp_ct_z = np.load('data/hcp_ct_z.npy')\n",
    "hcp_ct_resid = np.load('data/hcp_ct_resid.npy')\n",
    "hcp_ct_g = np.load('data/hcp_ct_g.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iho4wkAESgqf"
   },
   "outputs": [],
   "source": [
    "# generate train/test splits\n",
    "np.random.seed(42)\n",
    "n_train = int(0.9 * hcp_ct_z.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zi7v5e8vZ0Ms"
   },
   "outputs": [],
   "source": [
    "train_idxs = np.random.choice(range(hcp_ct_z.shape[0]), size=n_train, replace=False)\n",
    "test_idxs = np.array([x for x in range(hcp_ct_z.shape[0]) if x not in train_idxs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zi7v5e8vZ0Ms"
   },
   "outputs": [],
   "source": [
    "train_data_z = hcp_ct_z[train_idxs, :]\n",
    "test_data_z = hcp_ct_z[test_idxs, :]\n",
    "\n",
    "train_data_raw = hcp_ct_resid[train_idxs, :]\n",
    "test_data_raw = hcp_ct_resid[test_idxs, :]\n",
    "\n",
    "train_phen = hcp_ct_g[train_idxs]\n",
    "test_phen = hcp_ct_g[test_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean center train/test data (using train means)\n",
    "train_mu_centered_z = (train_data_z - train_data_z.mean(axis=0))\n",
    "test_mu_centered_z = (test_data_z - train_data_z.mean(axis=0))\n",
    "\n",
    "train_mu_centered_raw = (train_data_raw - train_data_raw.mean(axis=0))\n",
    "test_mu_centered_raw = (test_data_raw - train_data_raw.mean(axis=0))\n",
    "\n",
    "# from pca documentation, \"the input data is centered but not scaled for each feature before applying the SVD\"\n",
    "pca_model_z = decomposition.PCA(n_components=15).fit(train_data_z)\n",
    "pca_model_raw = decomposition.PCA(n_components=15).fit(train_data_raw)\n",
    "\n",
    "train_transformed_z = pca_model_z.transform(train_data_z)\n",
    "test_transformed_z = pca_model_z.transform(test_data_z)\n",
    "train_transformed_raw = pca_model_raw.transform(train_data_raw)\n",
    "test_transformed_raw = pca_model_raw.transform(test_data_raw)\n",
    "\n",
    "# OLS using sklearn\n",
    "lr_model_z = linear_model.LinearRegression(fit_intercept=True, normalize=False)\n",
    "lr_model_z.fit(train_transformed_z, train_phen)\n",
    "train_pred_phen_lr_model_z = lr_model_z.predict(train_transformed_z)\n",
    "test_pred_phen_lr_model_z = lr_model_z.predict(test_transformed_z)\n",
    "\n",
    "# OLS using sklearn\n",
    "lr_model_raw = linear_model.LinearRegression(fit_intercept=True, normalize=False)\n",
    "lr_model_raw.fit(train_transformed_raw, train_phen)\n",
    "train_pred_phen_lr_model_raw = lr_model_raw.predict(train_transformed_raw)\n",
    "test_pred_phen_lr_model_raw = lr_model_raw.predict(test_transformed_raw)\n",
    "\n",
    "# HCP Accuracy of Predictions (deviations)\n",
    "train_r2_z = metrics.r2_score(train_phen, train_pred_phen_lr_model_z)\n",
    "test_r2_z = metrics.r2_score(test_phen, test_pred_phen_lr_model_z)\n",
    "train_mse_z = metrics.mean_squared_error(train_phen, train_pred_phen_lr_model_z)\n",
    "test_mse_z = metrics.mean_squared_error(test_phen, test_pred_phen_lr_model_z)\n",
    "\n",
    "# HCP Accuracy of Predictions (raw)\n",
    "train_r2_raw = metrics.r2_score(train_phen, train_pred_phen_lr_model_raw)\n",
    "test_r2_raw = metrics.r2_score(test_phen, test_pred_phen_lr_model_raw)\n",
    "train_mse_raw = metrics.mean_squared_error(train_phen, train_pred_phen_lr_model_raw)\n",
    "test_mse_raw = metrics.mean_squared_error(test_phen, test_pred_phen_lr_model_raw)\n",
    "\n",
    "# Difference between deviation and raw\n",
    "diff_test_r = test_r2_z - test_r2_raw\n",
    "diff_test_mse = test_mse_z - test_mse_raw"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "other_predictive_models.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "braincharts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7 (default, Sep 16 2021, 08:50:36) \n[Clang 10.0.0 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "f65f66140ab2d9a57fedc58a3b7e1d01f34d12111107cec87dc46b07c8179a15"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
